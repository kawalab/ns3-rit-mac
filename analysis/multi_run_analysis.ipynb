{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f7a05e",
   "metadata": {},
   "source": [
    "## ns-3 RIT Scenario: Multi-Pattern Runs and Comparative Analysis (Seed-Averaged)\n",
    "\n",
    "This notebook runs **multiple ns-3 RIT simulation configurations** and performs\n",
    "**seed-averaged analysis for reliable comparison**.\n",
    "\n",
    "- **Outer layer**: Application and node-placement patterns\n",
    "- **Middle layer**: CSMA / MAC configuration patterns\n",
    "- **Inner layer**: Multiple random seeds for averaging\n",
    "- **Total runs**: outer × middle × seeds (e.g., 4 × 8 × 5 = 160)\n",
    "\n",
    "Simulation results are aggregated to compute **mean, standard deviation, and\n",
    "inter-seed variance** of key metrics such as PDR, latency, and wake-up time.\n",
    "\n",
    "All simulations are executed in parallel to efficiently handle seed averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f9f31",
   "metadata": {},
   "source": [
    "## Load Util Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from datetime import datetime\n",
    "\n",
    "from common.config_utils import SimulationConfig\n",
    "from common.ns3_utils import run_ns3_simulation, run_ns3_simulation_with_result\n",
    "from common.io_utils import read_log, find_node_dirs, remove_raw_logs\n",
    "from common.aggregate_utils import (\n",
    "    aggregate_app_summary,\n",
    "    aggregate_mac_summary,\n",
    "    aggregate_phy_summary,\n",
    "    aggregate_scenario_summary,\n",
    ")\n",
    "from common.log_constants import MAC_LOG_FILES, APP_TXLOG, APP_RXLOG\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(f\"Available CPU cores: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ba16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Execution control settings\n",
    "#\n",
    "# These options define how multi-run simulations are executed,\n",
    "# including rerun policy, error handling, log cleanup, and\n",
    "# verbosity.\n",
    "# ------------------------------------------------------------\n",
    "EXECUTION_CONFIG = {\n",
    "    'force_rerun': False,           # True: force rerun even if results exist\n",
    "                                    # False: skip if results already exist\n",
    "    'cleanup_logs': True,           # True: remove raw logs after successful aggregation\n",
    "                                    # False: keep raw logs\n",
    "    'continue_on_error': False,      # True: continue other tasks on error\n",
    "                                    # False: stop execution on first error\n",
    "    'allow_partial_results': True,   # True: allow partial results\n",
    "                                    # False: require complete results only\n",
    "    'save_error_log': True,          # True: save error logs\n",
    "                                    # False: do not save error logs\n",
    "    'verbose_progress': False        # True: detailed progress output\n",
    "                                    # False: minimal output\n",
    "}\n",
    "\n",
    "print(f\"Execution configuration: {EXECUTION_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab735020",
   "metadata": {},
   "source": [
    "## Scenario Configuraion (↓↓ edit here ↓↓)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7503406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------ ------------------\n",
    "# Scenario configuration\n",
    "# Define the 3-layer pattern set here:\n",
    "# - Outer layer: app + placement (+ optional beacon randomization), up to 8 patterns\n",
    "# - Middle layer: CSMA / PreCS variations, up to 20 patterns (configurable)\n",
    "# - Inner layer: seed list for averaging\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Outer patterns (up to 8 patterns) ---\n",
    "outer_patterns = {\n",
    "    \"periodic(center)\": {\n",
    "        \"App\": \"periodic\",\n",
    "        \"Placement\": \"center\"\n",
    "    },\n",
    "    \"periodic(edge)\": {\n",
    "        \"App\": \"periodic\",\n",
    "        \"Placement\": \"edge\"\n",
    "    },\n",
    "    \"random(center)\": {\n",
    "        \"App\": \"random\",\n",
    "        \"Placement\": \"center\"\n",
    "    },\n",
    "    \"random(edge)\": {\n",
    "        \"App\": \"random\",\n",
    "        \"Placement\": \"edge\"\n",
    "    },\n",
    "    \"periodic(center, brandom)\": {\n",
    "        \"App\": \"periodic\",\n",
    "        \"Placement\": \"center\",\n",
    "        \"BeaconRandomize\": \"true\"\n",
    "    },\n",
    "    \"periodic(edge, brandom)\": {\n",
    "        \"App\": \"periodic\",\n",
    "        \"Placement\": \"edge\",\n",
    "        \"BeaconRandomize\": \"true\"\n",
    "    },\n",
    "    \"random(center, brandom)\": {\n",
    "        \"App\": \"random\",\n",
    "        \"Placement\": \"center\",\n",
    "        \"BeaconRandomize\": \"true\"\n",
    "    },\n",
    "    \"random(edge, brandom)\": {\n",
    "        \"App\": \"random\",\n",
    "        \"Placement\": \"edge\",\n",
    "        \"BeaconRandomize\": \"true\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Middle patterns (CSMA / PreCS scenario variants) ---\n",
    "middle_scenario_configs = {\n",
    "    \"nn\": {\"params\": {}},\n",
    "    \"nn(back)\": {\"params\": {\"DWD\": 2, \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"np\": {\"params\": {\"BeaconPreCs\": \"true\"}},\n",
    "    \"np(back)\": {\"params\": {\"DWD\": 2, \"BeaconPreCs\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"pn\": {\"params\": {\"DataPreCs\": \"true\"}},\n",
    "    \"pn(back)\": {\"params\": {\"DWD\": 2, \"DataPreCs\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"pp\": {\"params\": {\"DataPreCs\": \"true\", \"BeaconPreCs\": \"true\"}},\n",
    "    \"pp(back)\": {\"params\": {\"DWD\": 2, \"DataPreCs\": \"true\", \"BeaconPreCs\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"cn\": {\"params\": {\"DataCsma\": \"true\"}},\n",
    "    \"cn(back)\": {\"params\": {\"DWD\": 2, \"DataCsma\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"nc\": {\"params\": {\"BeaconCsma\": \"true\"}},\n",
    "    \"nc(back)\": {\"params\": {\"DWD\": 2, \"BeaconCsma\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"pc\": {\"params\": {\"DataPreCs\": \"true\", \"BeaconCsma\": \"true\"}},\n",
    "    \"pc(back)\": {\"params\": {\"DWD\": 2, \"DataPreCs\": \"true\", \"BeaconCsma\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"cp\": {\"params\": {\"DataCsma\": \"true\", \"BeaconPreCs\": \"true\"}},\n",
    "    \"cp(back)\": {\"params\": {\"DWD\": 2, \"DataCsma\": \"true\", \"BeaconPreCs\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "\n",
    "    \"cc\": {\"params\": {\"DataCsma\": \"true\", \"BeaconCsma\": \"true\"}},\n",
    "    \"cc(back)\": {\"params\": {\"DWD\": 2, \"DataCsma\": \"true\", \"BeaconCsma\": \"true\", \"BeaconAck\": \"true\"}},\n",
    "}\n",
    "\n",
    "# --- Seed configuration (inner layer) ---\n",
    "seed_config = {\n",
    "    \"num_seeds\": 10,\n",
    "    \"base_seed\": 2000,\n",
    "    \"increment\": 1\n",
    "}\n",
    "\n",
    "seed_values = [\n",
    "    seed_config[\"base_seed\"] + i * seed_config[\"increment\"]\n",
    "    for i in range(seed_config[\"num_seeds\"])\n",
    "]\n",
    "\n",
    "# --- Base parameters (shared across all scenarios) ---\n",
    "base_params = {\n",
    "    \"BI\": 3000,\n",
    "    \"TWD\": 3000,\n",
    "    \"DWD\": 5,\n",
    "    \"Nodes\": -1,\n",
    "    \"Days\": 1,\n",
    "    \"DR\": 0,\n",
    "    \"AppPacketSize\": 8,\n",
    "    \"Seed\": 210,  # will be overwritten per-seed\n",
    "    \"DataCsma\": \"false\",\n",
    "    \"DataPreCs\": \"false\",\n",
    "    \"DataPreCsB\": \"false\",\n",
    "    \"BeaconCsma\": \"false\",\n",
    "    \"BeaconPreCs\": \"false\",\n",
    "    \"BeaconPreCsB\": \"false\",\n",
    "    \"ContinuousTx\": \"false\",\n",
    "    \"BeaconRandomize\": \"false\",\n",
    "    \"CompactRitDataRequest\": \"true\",\n",
    "    \"BeaconAck\": \"false\",\n",
    "    \"Placement\": \"edge\",\n",
    "    \"Density\": \"middle\",\n",
    "    \"App\": \"random\"\n",
    "}\n",
    "\n",
    "def merge_params(*param_dicts):\n",
    "    \"\"\"Merge multiple parameter dicts (later dicts override earlier ones).\"\"\"\n",
    "    merged = {}\n",
    "    for params in param_dicts:\n",
    "        merged.update(params)\n",
    "    return merged\n",
    "\n",
    "# --- Build the full 3-layer task list ---\n",
    "all_tasks = []\n",
    "scenario_matrix = {}\n",
    "base_script = \"scratch/rit-grid-converge.cc\"\n",
    "\n",
    "for outer_name, outer_params in outer_patterns.items():\n",
    "    scenario_matrix[outer_name] = {}\n",
    "\n",
    "    for middle_name, middle_config in middle_scenario_configs.items():\n",
    "        scenario_matrix[outer_name][middle_name] = {\"seed_tasks\": [], \"results\": []}\n",
    "\n",
    "        for seed_value in seed_values:\n",
    "            final_params = merge_params(\n",
    "                base_params,\n",
    "                middle_config[\"params\"],\n",
    "                outer_params,\n",
    "                {\"Seed\": seed_value}\n",
    "            )\n",
    "\n",
    "            task_name = f\"{outer_name}_{middle_name}_seed{seed_value}\"\n",
    "\n",
    "            task_info = {\n",
    "                \"task_name\": task_name,\n",
    "                \"params\": final_params,\n",
    "                \"base_script\": base_script,\n",
    "                \"outer_pattern\": outer_name,\n",
    "                \"middle_pattern\": middle_name,\n",
    "                \"seed_value\": seed_value\n",
    "            }\n",
    "\n",
    "            all_tasks.append(task_info)\n",
    "            scenario_matrix[outer_name][middle_name][\"seed_tasks\"].append(task_info)\n",
    "\n",
    "# --- Validation ---\n",
    "if len(outer_patterns) > 8:\n",
    "    raise ValueError(\"Outer pattern count must be <= 8\")\n",
    "if len(middle_scenario_configs) > 20:\n",
    "    raise ValueError(\"Middle pattern count must be <= 20\")\n",
    "if seed_config[\"num_seeds\"] <= 0:\n",
    "    raise ValueError(\"num_seeds must be >= 1\")\n",
    "if len(all_tasks) == 0:\n",
    "    raise ValueError(\"At least one task must be configured\")\n",
    "\n",
    "# --- Summary prints ---\n",
    "print(\"=== 3-layer pattern summary ===\")\n",
    "print(f\"Outer patterns: {len(outer_patterns)}\")\n",
    "print(f\"Middle patterns: {len(middle_scenario_configs)}\")\n",
    "print(f\"Seeds per scenario: {seed_config['num_seeds']}\")\n",
    "print(f\"Total tasks (outer × middle × seeds): {len(all_tasks)}\")\n",
    "print(f\"Scenario count (after seed-averaging): {len(outer_patterns) * len(middle_scenario_configs)}\")\n",
    "\n",
    "print(\"\\n=== Seed settings ===\")\n",
    "print(f\"Seed values: {seed_values}\")\n",
    "print(f\"Base seed: {seed_config['base_seed']}\")\n",
    "print(f\"Increment: {seed_config['increment']}\")\n",
    "\n",
    "print(\"\\n=== Outer patterns ===\")\n",
    "for name, params in outer_patterns.items():\n",
    "    print(f\"{name}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Middle patterns ===\")\n",
    "for name, config in middle_scenario_configs.items():\n",
    "    print(f\"{name}:\")\n",
    "    if config[\"params\"]:\n",
    "        for key, value in config[\"params\"].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"  (same as base params)\")\n",
    "    print()\n",
    "\n",
    "print(\"=== Base params ===\")\n",
    "for key, value in base_params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Existing-results check & error-tolerance utilities\n",
    "\n",
    "def is_valid_csv(file_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether a CSV file is valid.\n",
    "\n",
    "    A CSV is considered valid if:\n",
    "      - the file exists\n",
    "      - the file size is non-zero\n",
    "      - it can be read by pandas\n",
    "      - it contains at least one row\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the CSV is valid; otherwise False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            return False\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "        return len(df) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_existing_task_results(task_info: dict, force_rerun: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Check whether an existing task result is complete and can be skipped.\n",
    "\n",
    "    Args:\n",
    "        task_info (dict): Task metadata.\n",
    "        force_rerun (bool): If True, do not skip even if results exist.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'should_skip': bool,            # True if all required files are valid\n",
    "            'existing_files': list[str],    # list of valid existing files\n",
    "            'missing_files': list[str],     # list of missing/invalid files\n",
    "            'file_status': dict,            # per-file detailed status\n",
    "            'reason': str                   # short reason message\n",
    "        }\n",
    "    \"\"\"\n",
    "    if force_rerun:\n",
    "        return {\n",
    "            'should_skip': False,\n",
    "            'existing_files': [],\n",
    "            'missing_files': [\n",
    "                'app_summary.csv',\n",
    "                'mac_summary.csv',\n",
    "                'phy_summary.csv',\n",
    "                'scenario_summary.csv'\n",
    "            ],\n",
    "            'file_status': {},\n",
    "            'reason': 'force_rerun=True'\n",
    "        }\n",
    "\n",
    "    config = SimulationConfig(task_info[\"params\"], task_info[\"base_script\"])\n",
    "\n",
    "    required_files = {\n",
    "        'app_summary.csv': config.get_summary_path(\"app_summary.csv\"),\n",
    "        'mac_summary.csv': config.get_summary_path(\"mac_summary.csv\"),\n",
    "        'phy_summary.csv': config.get_summary_path(\"phy_summary.csv\"),\n",
    "        'scenario_summary.csv': config.get_summary_path(\"scenario_summary.csv\")\n",
    "    }\n",
    "\n",
    "    file_status = {}\n",
    "\n",
    "    for name, path in required_files.items():\n",
    "        status = {\n",
    "            'path': path,\n",
    "            'exists': False,\n",
    "            'readable': False,\n",
    "            'has_data': False,\n",
    "            'file_size': 0,\n",
    "            'row_count': 0,\n",
    "            'error': None\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                status['error'] = \"file does not exist\"\n",
    "                file_status[name] = status\n",
    "                continue\n",
    "\n",
    "            status['exists'] = True\n",
    "            status['file_size'] = os.path.getsize(path)\n",
    "\n",
    "            if status['file_size'] == 0:\n",
    "                status['error'] = \"empty file\"\n",
    "                file_status[name] = status\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                status['readable'] = True\n",
    "                status['row_count'] = len(df)\n",
    "                status['has_data'] = len(df) > 0\n",
    "                if not status['has_data']:\n",
    "                    status['error'] = \"no rows\"\n",
    "            except Exception as csv_error:\n",
    "                status['readable'] = False\n",
    "                status['error'] = f\"CSV read error: {csv_error}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            status['error'] = f\"file access error: {e}\"\n",
    "\n",
    "        file_status[name] = status\n",
    "\n",
    "    all_required_valid = all(\n",
    "        st['exists'] and st['readable'] and st['has_data']\n",
    "        for st in file_status.values()\n",
    "    )\n",
    "\n",
    "    existing_files = [\n",
    "        name for name, st in file_status.items()\n",
    "        if st['exists'] and st['readable'] and st['has_data']\n",
    "    ]\n",
    "    missing_files = [\n",
    "        name for name, st in file_status.items()\n",
    "        if not (st['exists'] and st['readable'] and st['has_data'])\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'should_skip': all_required_valid,\n",
    "        'existing_files': existing_files,\n",
    "        'missing_files': missing_files,\n",
    "        'file_status': file_status,\n",
    "        'reason': 'all required summaries exist and are valid' if all_required_valid else 'missing/invalid summary files'\n",
    "    }\n",
    "\n",
    "\n",
    "def load_existing_results(task_info: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Load existing results and return them in the same format as a normal task result.\n",
    "\n",
    "    Args:\n",
    "        task_info (dict): Task metadata.\n",
    "\n",
    "    Returns:\n",
    "        dict: Task-result dict compatible with run_ns3_simulation_with_result outputs.\n",
    "    \"\"\"\n",
    "    config = SimulationConfig(task_info[\"params\"], task_info[\"base_script\"])\n",
    "\n",
    "    try:\n",
    "        scenario_path = config.get_summary_path(\"scenario_summary.csv\")\n",
    "        scenario_df = pd.read_csv(scenario_path)\n",
    "\n",
    "        scenario_summary = scenario_df.iloc[0].to_dict() if len(scenario_df) > 0 else {}\n",
    "\n",
    "        return {\n",
    "            'success': True,\n",
    "            'task_name': task_info[\"task_name\"],\n",
    "            'summary': scenario_summary,\n",
    "            'outer_pattern': task_info[\"outer_pattern\"],\n",
    "            'middle_pattern': task_info[\"middle_pattern\"],\n",
    "            'seed_value': task_info[\"seed_value\"],\n",
    "            'skipped': True,\n",
    "            'error': None\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'task_name': task_info[\"task_name\"],\n",
    "            'summary': None,\n",
    "            'outer_pattern': task_info[\"outer_pattern\"],\n",
    "            'middle_pattern': task_info[\"middle_pattern\"],\n",
    "            'seed_value': task_info[\"seed_value\"],\n",
    "            'skipped': False,\n",
    "            'error': f\"failed to load existing results: {e}\"\n",
    "        }\n",
    "\n",
    "print(\"Existing-results check utilities loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_with_config(outer_pattern, middle_pattern, seed):\n",
    "    \"\"\"\n",
    "    Run a single ns-3 simulation for the given (outer, middle, seed) configuration.\n",
    "    \"\"\"\n",
    "    # Merge parameters (base + middle overrides + outer overrides + seed)\n",
    "    final_params = merge_params(\n",
    "        base_params,\n",
    "        middle_scenario_configs[middle_pattern][\"params\"],\n",
    "        outer_patterns[outer_pattern],\n",
    "        {\"Seed\": seed}\n",
    "    )\n",
    "\n",
    "    # Build SimulationConfig and run\n",
    "    config = SimulationConfig(final_params, base_script)\n",
    "    return run_ns3_simulation_with_result(config)\n",
    "\n",
    "\n",
    "def process_simulation_statistics(outer_pattern, middle_pattern, seed):\n",
    "    \"\"\"\n",
    "    Run post-processing (aggregation) for a single simulation result.\n",
    "    Generates CSV summaries and returns in-memory summaries.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import traceback\n",
    "\n",
    "    # Merge parameters\n",
    "    final_params = merge_params(\n",
    "        base_params,\n",
    "        middle_scenario_configs[middle_pattern][\"params\"],\n",
    "        outer_patterns[outer_pattern],\n",
    "        {\"Seed\": seed}\n",
    "    )\n",
    "\n",
    "    config = SimulationConfig(final_params, base_script)\n",
    "\n",
    "    try:\n",
    "        print(f\"Start statistics: {outer_pattern}_{middle_pattern}_seed{seed}\")\n",
    "\n",
    "        base_dir = os.path.join(config.ns3_working_dir, \"logs\")\n",
    "        parameter_dir = config.parameter_dir\n",
    "        log_base_path = os.path.join(base_dir, parameter_dir)\n",
    "\n",
    "        print(f\"  Log directory: {log_base_path}\")\n",
    "\n",
    "        if not os.path.exists(log_base_path):\n",
    "            print(f\"  Error: log directory not found: {log_base_path}\")\n",
    "            return {'success': False, 'error': f\"Log directory not found: {log_base_path}\"}\n",
    "\n",
    "        # Detect node directories\n",
    "        existing_nodes = []\n",
    "        for item in os.listdir(log_base_path):\n",
    "            full = os.path.join(log_base_path, item)\n",
    "            if item.startswith(\"node-\") and os.path.isdir(full):\n",
    "                try:\n",
    "                    existing_nodes.append(int(item.split(\"-\")[1]))\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "\n",
    "        existing_nodes.sort()\n",
    "        print(f\"  Detected nodes: {existing_nodes}\")\n",
    "\n",
    "        if not existing_nodes:\n",
    "            print(\"  Error: no valid node directories found\")\n",
    "            return {'success': False, 'error': \"No valid node directories found\"}\n",
    "\n",
    "        APP_RECV_NODE = 0\n",
    "        APP_SEND_NODES = [n for n in existing_nodes if n != APP_RECV_NODE]\n",
    "        MAC_NODES = existing_nodes\n",
    "\n",
    "        # --- App summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating APP...\")\n",
    "            app_summary = aggregate_app_summary(\n",
    "                APP_SEND_NODES, APP_RECV_NODE, APP_TXLOG, APP_RXLOG, base_dir, parameter_dir\n",
    "            )\n",
    "            app_csv_path = config.get_summary_path(\"app_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(app_csv_path), exist_ok=True)\n",
    "            app_summary.to_csv(app_csv_path, index=False)\n",
    "            print(\"  APP done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  APP error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"App summary error: {e}\"}\n",
    "\n",
    "        # --- MAC summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating MAC...\")\n",
    "            mac_summary = aggregate_mac_summary(\n",
    "                MAC_NODES, MAC_LOG_FILES, base_dir, parameter_dir\n",
    "            )\n",
    "            mac_csv_path = config.get_summary_path(\"mac_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(mac_csv_path), exist_ok=True)\n",
    "            mac_summary.to_csv(mac_csv_path, index=False)\n",
    "            print(\"  MAC done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  MAC error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"MAC summary error: {e}\"}\n",
    "\n",
    "        # --- PHY summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating PHY...\")\n",
    "            phy_summary = aggregate_phy_summary(\n",
    "                MAC_NODES, base_dir, parameter_dir\n",
    "            )\n",
    "            phy_csv_path = config.get_summary_path(\"phy_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(phy_csv_path), exist_ok=True)\n",
    "            phy_summary.to_csv(phy_csv_path, index=False)\n",
    "            print(\"  PHY done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  PHY error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"PHY summary error: {e}\"}\n",
    "\n",
    "        # --- Scenario summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating SCENARIO...\")\n",
    "            scenario_summary = aggregate_scenario_summary(app_summary, phy_summary)\n",
    "            scenario_summary_df = pd.DataFrame([scenario_summary])\n",
    "            scenario_csv_path = config.get_summary_path(\"scenario_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(scenario_csv_path), exist_ok=True)\n",
    "            scenario_summary_df.to_csv(scenario_csv_path, index=False)\n",
    "            print(\"  SCENARIO done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  SCENARIO error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"Scenario summary error: {e}\"}\n",
    "\n",
    "        print(f\"Statistics success: {outer_pattern}_{middle_pattern}_seed{seed}\")\n",
    "        return {\n",
    "            'success': True,\n",
    "            'app_summary': app_summary,\n",
    "            'mac_summary': mac_summary,\n",
    "            'phy_summary': phy_summary,\n",
    "            'scenario_summary': scenario_summary\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected statistics error: {outer_pattern}_{middle_pattern}_seed{seed}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {'success': False, 'error': f\"Unexpected statistics error: {e}\"}\n",
    "\n",
    "\n",
    "def run_single_task_robust(task_info):\n",
    "    \"\"\"\n",
    "    Robust single-task runner.\n",
    "\n",
    "    Features:\n",
    "      1) Auto-skip if valid existing summaries are found.\n",
    "      2) Optionally remove raw logs after successful aggregation.\n",
    "      3) Keep a structured result dict even on failures (no sys.exit).\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    task_name = task_info[\"task_name\"]\n",
    "    outer_pattern = task_info[\"outer_pattern\"]\n",
    "    middle_pattern = task_info[\"middle_pattern\"]\n",
    "    seed_value = task_info[\"seed_value\"]\n",
    "\n",
    "    # Existing result check (skip if complete)\n",
    "    existing_check = check_existing_task_results(\n",
    "        task_info,\n",
    "        EXECUTION_CONFIG.get(\"force_rerun\", False)\n",
    "    )\n",
    "    if existing_check[\"should_skip\"]:\n",
    "        return load_existing_results(task_info)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'task_name': task_name,\n",
    "        'summary': None,\n",
    "        'outer_pattern': outer_pattern,\n",
    "        'middle_pattern': middle_pattern,\n",
    "        'seed_value': seed_value,\n",
    "        'skipped': False,\n",
    "        'partial_success': False,\n",
    "        'cleanup_status': 'not_attempted',\n",
    "        'execution_time': 0,\n",
    "        'error': None,\n",
    "        'simulation_success': False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "            print(f\"Start: {task_name}\")\n",
    "\n",
    "        # --- Run simulation ---\n",
    "        try:\n",
    "            _ = run_simulation_with_config(outer_pattern, middle_pattern, seed_value)\n",
    "            result['simulation_success'] = True\n",
    "        except Exception as sim_error:\n",
    "            # IMPORTANT: do NOT sys.exit() here (would kill the whole pool/job)\n",
    "            result['simulation_success'] = False\n",
    "            result['error'] = f\"Simulation error: {sim_error}\"\n",
    "            if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                print(f\"Simulation failed: {task_name} ({sim_error})\")\n",
    "            return result\n",
    "\n",
    "        # --- Run statistics ---\n",
    "        try:\n",
    "            stats_result = process_simulation_statistics(outer_pattern, middle_pattern, seed_value)\n",
    "            if stats_result.get('success', False):\n",
    "                result['success'] = True\n",
    "                result['summary'] = stats_result['scenario_summary']\n",
    "\n",
    "                # Cleanup raw logs only after success\n",
    "                if EXECUTION_CONFIG.get('cleanup_logs', False):\n",
    "                    try:\n",
    "                        final_params = merge_params(\n",
    "                            base_params,\n",
    "                            middle_scenario_configs[middle_pattern][\"params\"],\n",
    "                            outer_patterns[outer_pattern],\n",
    "                            {\"Seed\": seed_value}\n",
    "                        )\n",
    "                        config = SimulationConfig(final_params, base_script)\n",
    "                        log_base_dir = os.path.join(config.ns3_working_dir, \"logs\")\n",
    "\n",
    "                        remove_raw_logs(log_base_dir, config.parameter_dir)\n",
    "                        result['cleanup_status'] = 'completed'\n",
    "                    except Exception as cleanup_error:\n",
    "                        result['cleanup_status'] = 'error'\n",
    "                        if EXECUTION_CONFIG.get('verbose_progress', False):\n",
    "                            print(f\"Cleanup error: {task_name} ({cleanup_error})\")\n",
    "\n",
    "                if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                    print(f\"Done: {task_name}\")\n",
    "\n",
    "            else:\n",
    "                result['partial_success'] = True\n",
    "                result['error'] = f\"Statistics error: {stats_result.get('error', 'unknown error')}\"\n",
    "                if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                    print(f\"Statistics failed: {task_name} ({result['error']})\")\n",
    "\n",
    "        except Exception as stats_error:\n",
    "            result['partial_success'] = True\n",
    "            result['error'] = f\"Statistics processing error: {stats_error}\"\n",
    "            if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                print(f\"Statistics exception: {task_name} ({stats_error})\")\n",
    "\n",
    "    except Exception as unexpected_error:\n",
    "        result['error'] = f\"Unexpected error: {unexpected_error}\"\n",
    "        if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "            print(f\"Unexpected error: {task_name} ({unexpected_error})\")\n",
    "\n",
    "    finally:\n",
    "        result['execution_time'] = time.time() - start_time\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_execution_report(execution_results):\n",
    "    \"\"\"\n",
    "    Generate a detailed execution report from task results.\n",
    "    \"\"\"\n",
    "    if not execution_results:\n",
    "        print(\"No execution results.\")\n",
    "        return {}\n",
    "\n",
    "    total_tasks = len(execution_results)\n",
    "\n",
    "    success_count = len([r for r in execution_results if r.get('success', False)])\n",
    "    skipped_count = len([r for r in execution_results if r.get('skipped', False)])\n",
    "    failed_count = total_tasks - success_count - skipped_count\n",
    "    partial_count = len([r for r in execution_results if r.get('partial_success', False)])\n",
    "\n",
    "    execution_times = [\n",
    "        r.get('execution_time', 0)\n",
    "        for r in execution_results\n",
    "        if not r.get('skipped', False)\n",
    "    ]\n",
    "\n",
    "    cleanup_completed = len([r for r in execution_results if r.get('cleanup_status') == 'completed'])\n",
    "    cleanup_failed = len([r for r in execution_results if r.get('cleanup_status') == 'error'])\n",
    "\n",
    "    print(\"\\n=== Execution Report ===\")\n",
    "    print(f\"Total tasks: {total_tasks}\")\n",
    "    print(f\"Success: {success_count} ({success_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Skipped: {skipped_count} ({skipped_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Failed: {failed_count} ({failed_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Partial success: {partial_count} ({partial_count/total_tasks*100:.1f}%)\")\n",
    "\n",
    "    if execution_times:\n",
    "        print(\"\\nExecution time stats:\")\n",
    "        print(f\"  Total: {sum(execution_times):.1f}s\")\n",
    "        print(f\"  Mean: {np.mean(execution_times):.1f}s\")\n",
    "        print(f\"  Max: {max(execution_times):.1f}s\")\n",
    "        print(f\"  Min: {min(execution_times):.1f}s\")\n",
    "\n",
    "    if EXECUTION_CONFIG.get('cleanup_logs', False):\n",
    "        print(\"\\nRaw-log cleanup stats:\")\n",
    "        print(f\"  Completed: {cleanup_completed}\")\n",
    "        print(f\"  Failed: {cleanup_failed}\")\n",
    "\n",
    "    return {\n",
    "        'total_tasks': total_tasks,\n",
    "        'success_count': success_count,\n",
    "        'skipped_count': skipped_count,\n",
    "        'failed_count': failed_count,\n",
    "        'partial_count': partial_count,\n",
    "        'execution_times': execution_times,\n",
    "        'cleanup_completed': cleanup_completed,\n",
    "        'cleanup_failed': cleanup_failed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c285ff",
   "metadata": {},
   "source": [
    "## Run parallell simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b65eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution settings (fault-tolerant / skip support / raw log cleanup)\n",
    "max_workers = min(len(all_tasks), cpu_count() - 1)  # Use CPU-1 for stable operation\n",
    "if max_workers <= 0:\n",
    "    max_workers = 1\n",
    "\n",
    "print(\"=== Parallel Execution Settings ===\")\n",
    "print(f\"Total tasks: {len(all_tasks)}\")\n",
    "print(f\"Available CPUs: {cpu_count()}\")\n",
    "print(f\"Worker processes: {max_workers}\")\n",
    "print(\"Execution control options:\")\n",
    "for key, value in EXECUTION_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Start time: {datetime.now()}\")\n",
    "\n",
    "# Fault-tolerant parallel execution with skip support\n",
    "try:\n",
    "    with Pool(max_workers) as pool:\n",
    "        task_results = pool.map(run_single_task_robust, all_tasks)\n",
    "\n",
    "    print(f\"\\nAll tasks completed: {datetime.now()}\")\n",
    "\n",
    "    # Generate an execution summary report\n",
    "    generate_execution_report(task_results)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during parallel execution: {str(e)}\")\n",
    "    if not EXECUTION_CONFIG[\"continue_on_error\"]:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_with_config(outer_pattern, middle_pattern, seed):\n",
    "    \"\"\"\n",
    "    Run a single ns-3 simulation for the given (outer, middle, seed) configuration.\n",
    "    \"\"\"\n",
    "    # Merge parameters (base + middle overrides + outer overrides + seed)\n",
    "    final_params = merge_params(\n",
    "        base_params,\n",
    "        middle_scenario_configs[middle_pattern][\"params\"],\n",
    "        outer_patterns[outer_pattern],\n",
    "        {\"Seed\": seed}\n",
    "    )\n",
    "\n",
    "    # Build SimulationConfig and run\n",
    "    config = SimulationConfig(final_params, base_script)\n",
    "    return run_ns3_simulation_with_result(config)\n",
    "\n",
    "\n",
    "def process_simulation_statistics(outer_pattern, middle_pattern, seed):\n",
    "    \"\"\"\n",
    "    Run post-processing (aggregation) for a single simulation result.\n",
    "    Generates CSV summaries and returns in-memory summaries.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import traceback\n",
    "\n",
    "    # Merge parameters\n",
    "    final_params = merge_params(\n",
    "        base_params,\n",
    "        middle_scenario_configs[middle_pattern][\"params\"],\n",
    "        outer_patterns[outer_pattern],\n",
    "        {\"Seed\": seed}\n",
    "    )\n",
    "\n",
    "    config = SimulationConfig(final_params, base_script)\n",
    "\n",
    "    try:\n",
    "        print(f\"Start statistics: {outer_pattern}_{middle_pattern}_seed{seed}\")\n",
    "\n",
    "        base_dir = os.path.join(config.ns3_working_dir, \"logs\")\n",
    "        parameter_dir = config.parameter_dir\n",
    "        log_base_path = os.path.join(base_dir, parameter_dir)\n",
    "\n",
    "        print(f\"  Log directory: {log_base_path}\")\n",
    "\n",
    "        if not os.path.exists(log_base_path):\n",
    "            print(f\"  Error: log directory not found: {log_base_path}\")\n",
    "            return {'success': False, 'error': f\"Log directory not found: {log_base_path}\"}\n",
    "\n",
    "        # Detect node directories\n",
    "        existing_nodes = []\n",
    "        for item in os.listdir(log_base_path):\n",
    "            full = os.path.join(log_base_path, item)\n",
    "            if item.startswith(\"node-\") and os.path.isdir(full):\n",
    "                try:\n",
    "                    existing_nodes.append(int(item.split(\"-\")[1]))\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "\n",
    "        existing_nodes.sort()\n",
    "        print(f\"  Detected nodes: {existing_nodes}\")\n",
    "\n",
    "        if not existing_nodes:\n",
    "            print(\"  Error: no valid node directories found\")\n",
    "            return {'success': False, 'error': \"No valid node directories found\"}\n",
    "\n",
    "        APP_RECV_NODE = 0\n",
    "        APP_SEND_NODES = [n for n in existing_nodes if n != APP_RECV_NODE]\n",
    "        MAC_NODES = existing_nodes\n",
    "\n",
    "        # --- App summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating APP...\")\n",
    "            app_summary = aggregate_app_summary(\n",
    "                APP_SEND_NODES, APP_RECV_NODE, APP_TXLOG, APP_RXLOG, base_dir, parameter_dir\n",
    "            )\n",
    "            app_csv_path = config.get_summary_path(\"app_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(app_csv_path), exist_ok=True)\n",
    "            app_summary.to_csv(app_csv_path, index=False)\n",
    "            print(\"  APP done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  APP error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"App summary error: {e}\"}\n",
    "\n",
    "        # --- MAC summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating MAC...\")\n",
    "            mac_summary = aggregate_mac_summary(\n",
    "                MAC_NODES, MAC_LOG_FILES, base_dir, parameter_dir\n",
    "            )\n",
    "            mac_csv_path = config.get_summary_path(\"mac_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(mac_csv_path), exist_ok=True)\n",
    "            mac_summary.to_csv(mac_csv_path, index=False)\n",
    "            print(\"  MAC done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  MAC error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"MAC summary error: {e}\"}\n",
    "\n",
    "        # --- PHY summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating PHY...\")\n",
    "            phy_summary = aggregate_phy_summary(\n",
    "                MAC_NODES, base_dir, parameter_dir\n",
    "            )\n",
    "            phy_csv_path = config.get_summary_path(\"phy_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(phy_csv_path), exist_ok=True)\n",
    "            phy_summary.to_csv(phy_csv_path, index=False)\n",
    "            print(\"  PHY done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  PHY error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"PHY summary error: {e}\"}\n",
    "\n",
    "        # --- Scenario summary ---\n",
    "        try:\n",
    "            print(\"  Aggregating SCENARIO...\")\n",
    "            scenario_summary = aggregate_scenario_summary(app_summary, phy_summary)\n",
    "            scenario_summary_df = pd.DataFrame([scenario_summary])\n",
    "            scenario_csv_path = config.get_summary_path(\"scenario_summary.csv\")\n",
    "            os.makedirs(os.path.dirname(scenario_csv_path), exist_ok=True)\n",
    "            scenario_summary_df.to_csv(scenario_csv_path, index=False)\n",
    "            print(\"  SCENARIO done.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  SCENARIO error: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return {'success': False, 'error': f\"Scenario summary error: {e}\"}\n",
    "\n",
    "        print(f\"Statistics success: {outer_pattern}_{middle_pattern}_seed{seed}\")\n",
    "        return {\n",
    "            'success': True,\n",
    "            'app_summary': app_summary,\n",
    "            'mac_summary': mac_summary,\n",
    "            'phy_summary': phy_summary,\n",
    "            'scenario_summary': scenario_summary\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected statistics error: {outer_pattern}_{middle_pattern}_seed{seed}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {'success': False, 'error': f\"Unexpected statistics error: {e}\"}\n",
    "\n",
    "\n",
    "def run_single_task_robust(task_info):\n",
    "    \"\"\"\n",
    "    Robust single-task runner.\n",
    "\n",
    "    Features:\n",
    "      1) Auto-skip if valid existing summaries are found.\n",
    "      2) Optionally remove raw logs after successful aggregation.\n",
    "      3) Keep a structured result dict even on failures (no sys.exit).\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    task_name = task_info[\"task_name\"]\n",
    "    outer_pattern = task_info[\"outer_pattern\"]\n",
    "    middle_pattern = task_info[\"middle_pattern\"]\n",
    "    seed_value = task_info[\"seed_value\"]\n",
    "\n",
    "    # Existing result check (skip if complete)\n",
    "    existing_check = check_existing_task_results(\n",
    "        task_info,\n",
    "        EXECUTION_CONFIG.get(\"force_rerun\", False)\n",
    "    )\n",
    "    if existing_check[\"should_skip\"]:\n",
    "        return load_existing_results(task_info)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = {\n",
    "        'success': False,\n",
    "        'task_name': task_name,\n",
    "        'summary': None,\n",
    "        'outer_pattern': outer_pattern,\n",
    "        'middle_pattern': middle_pattern,\n",
    "        'seed_value': seed_value,\n",
    "        'skipped': False,\n",
    "        'partial_success': False,\n",
    "        'cleanup_status': 'not_attempted',\n",
    "        'execution_time': 0,\n",
    "        'error': None,\n",
    "        'simulation_success': False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "            print(f\"Start: {task_name}\")\n",
    "\n",
    "        # --- Run simulation ---\n",
    "        try:\n",
    "            _ = run_simulation_with_config(outer_pattern, middle_pattern, seed_value)\n",
    "            result['simulation_success'] = True\n",
    "        except Exception as sim_error:\n",
    "            # IMPORTANT: do NOT sys.exit() here (would kill the whole pool/job)\n",
    "            result['simulation_success'] = False\n",
    "            result['error'] = f\"Simulation error: {sim_error}\"\n",
    "            if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                print(f\"Simulation failed: {task_name} ({sim_error})\")\n",
    "            return result\n",
    "\n",
    "        # --- Run statistics ---\n",
    "        try:\n",
    "            stats_result = process_simulation_statistics(outer_pattern, middle_pattern, seed_value)\n",
    "            if stats_result.get('success', False):\n",
    "                result['success'] = True\n",
    "                result['summary'] = stats_result['scenario_summary']\n",
    "\n",
    "                # Cleanup raw logs only after success\n",
    "                if EXECUTION_CONFIG.get('cleanup_logs', False):\n",
    "                    try:\n",
    "                        final_params = merge_params(\n",
    "                            base_params,\n",
    "                            middle_scenario_configs[middle_pattern][\"params\"],\n",
    "                            outer_patterns[outer_pattern],\n",
    "                            {\"Seed\": seed_value}\n",
    "                        )\n",
    "                        config = SimulationConfig(final_params, base_script)\n",
    "                        log_base_dir = os.path.join(config.ns3_working_dir, \"logs\")\n",
    "\n",
    "                        remove_raw_logs(log_base_dir, config.parameter_dir)\n",
    "                        result['cleanup_status'] = 'completed'\n",
    "                    except Exception as cleanup_error:\n",
    "                        result['cleanup_status'] = 'error'\n",
    "                        if EXECUTION_CONFIG.get('verbose_progress', False):\n",
    "                            print(f\"Cleanup error: {task_name} ({cleanup_error})\")\n",
    "\n",
    "                if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                    print(f\"Done: {task_name}\")\n",
    "\n",
    "            else:\n",
    "                result['partial_success'] = True\n",
    "                result['error'] = f\"Statistics error: {stats_result.get('error', 'unknown error')}\"\n",
    "                if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                    print(f\"Statistics failed: {task_name} ({result['error']})\")\n",
    "\n",
    "        except Exception as stats_error:\n",
    "            result['partial_success'] = True\n",
    "            result['error'] = f\"Statistics processing error: {stats_error}\"\n",
    "            if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "                print(f\"Statistics exception: {task_name} ({stats_error})\")\n",
    "\n",
    "    except Exception as unexpected_error:\n",
    "        result['error'] = f\"Unexpected error: {unexpected_error}\"\n",
    "        if EXECUTION_CONFIG.get(\"verbose_progress\", False):\n",
    "            print(f\"Unexpected error: {task_name} ({unexpected_error})\")\n",
    "\n",
    "    finally:\n",
    "        result['execution_time'] = time.time() - start_time\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_execution_report(execution_results):\n",
    "    \"\"\"\n",
    "    Generate a detailed execution report from task results.\n",
    "    \"\"\"\n",
    "    if not execution_results:\n",
    "        print(\"No execution results.\")\n",
    "        return {}\n",
    "\n",
    "    total_tasks = len(execution_results)\n",
    "\n",
    "    success_count = len([r for r in execution_results if r.get('success', False)])\n",
    "    skipped_count = len([r for r in execution_results if r.get('skipped', False)])\n",
    "    failed_count = total_tasks - success_count - skipped_count\n",
    "    partial_count = len([r for r in execution_results if r.get('partial_success', False)])\n",
    "\n",
    "    execution_times = [\n",
    "        r.get('execution_time', 0)\n",
    "        for r in execution_results\n",
    "        if not r.get('skipped', False)\n",
    "    ]\n",
    "\n",
    "    cleanup_completed = len([r for r in execution_results if r.get('cleanup_status') == 'completed'])\n",
    "    cleanup_failed = len([r for r in execution_results if r.get('cleanup_status') == 'error'])\n",
    "\n",
    "    print(\"\\n=== Execution Report ===\")\n",
    "    print(f\"Total tasks: {total_tasks}\")\n",
    "    print(f\"Success: {success_count} ({success_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Skipped: {skipped_count} ({skipped_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Failed: {failed_count} ({failed_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Partial success: {partial_count} ({partial_count/total_tasks*100:.1f}%)\")\n",
    "\n",
    "    if execution_times:\n",
    "        print(\"\\nExecution time stats:\")\n",
    "        print(f\"  Total: {sum(execution_times):.1f}s\")\n",
    "        print(f\"  Mean: {np.mean(execution_times):.1f}s\")\n",
    "        print(f\"  Max: {max(execution_times):.1f}s\")\n",
    "        print(f\"  Min: {min(execution_times):.1f}s\")\n",
    "\n",
    "    if EXECUTION_CONFIG.get('cleanup_logs', False):\n",
    "        print(\"\\nRaw-log cleanup stats:\")\n",
    "        print(f\"  Completed: {cleanup_completed}\")\n",
    "        print(f\"  Failed: {cleanup_failed}\")\n",
    "\n",
    "    return {\n",
    "        'total_tasks': total_tasks,\n",
    "        'success_count': success_count,\n",
    "        'skipped_count': skipped_count,\n",
    "        'failed_count': failed_count,\n",
    "        'partial_count': partial_count,\n",
    "        'execution_times': execution_times,\n",
    "        'cleanup_completed': cleanup_completed,\n",
    "        'cleanup_failed': cleanup_failed\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea2e97",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-layer pattern comparison plots (seed-averaged)\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization depends on the number of outer patterns\n",
    "num_outer = len(outer_patterns)\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Noto Sans CJK JP']\n",
    "\n",
    "if num_outer == 1:\n",
    "    # Single outer pattern: compare middle patterns\n",
    "    print(\"=== Middle-pattern comparison plots (seed-averaged) ===\")\n",
    "\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        f'Middle-pattern comparison (seed-averaged) ({list(outer_patterns.keys())[0]})',\n",
    "        fontsize=16,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "    # Color palette\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(middle_scenario_configs)))\n",
    "\n",
    "    # Extract results for middle patterns only\n",
    "    middle_df = comparison_df.copy()\n",
    "\n",
    "    # Plot 1: PDR mean\n",
    "    axes[0, 0].bar(middle_df['middle_pattern'], middle_df['pdr_mean'], color=colors)\n",
    "    axes[0, 0].set_title('PDR mean (seed-averaged)', fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('PDR')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: PDR standard deviation\n",
    "    axes[0, 1].bar(middle_df['middle_pattern'], middle_df['pdr_std'], color=colors)\n",
    "    axes[0, 1].set_title('PDR std (seed-averaged)', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('PDR std')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Delay mean\n",
    "    axes[0, 2].bar(middle_df['middle_pattern'], middle_df['delay_mean'], color=colors)\n",
    "    axes[0, 2].set_title('Delay mean (seed-averaged)', fontweight='bold')\n",
    "    axes[0, 2].set_ylabel('Delay (ms)')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Delay standard deviation\n",
    "    axes[1, 0].bar(middle_df['middle_pattern'], middle_df['delay_std'], color=colors)\n",
    "    axes[1, 0].set_title('Delay std (seed-averaged)', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Delay std (ms)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: Wake ratio mean\n",
    "    axes[1, 1].bar(middle_df['middle_pattern'], middle_df['wake_ratio_mean'], color=colors)\n",
    "    axes[1, 1].set_title('Wake ratio mean (seed-averaged)', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Wake ratio')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 6: Wake ratio standard deviation\n",
    "    axes[1, 2].bar(middle_df['middle_pattern'], middle_df['wake_ratio_std'], color=colors)\n",
    "    axes[1, 2].set_title('Wake ratio std (seed-averaged)', fontweight='bold')\n",
    "    axes[1, 2].set_ylabel('Wake ratio std')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    chart_path = \"~/workspace/analysis/middle_pattern_seed_averaged_comparison.png\"\n",
    "    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # Multiple outer patterns: three-layer comparison\n",
    "    print(\"=== Three-layer pattern comparison (seed-averaged) ===\")\n",
    "\n",
    "    # 1. Performance comparison by outer pattern\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Performance comparison by outer pattern (seed-averaged)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Heatmap: PDR mean\n",
    "    pdr_pivot = comparison_df.pivot(index='middle_pattern', columns='outer_pattern', values='pdr_mean')\n",
    "    sns.heatmap(pdr_pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('PDR mean (seed-averaged)')\n",
    "\n",
    "    # Heatmap: Delay mean\n",
    "    delay_pivot = comparison_df.pivot(index='middle_pattern', columns='outer_pattern', values='delay_mean')\n",
    "    sns.heatmap(delay_pivot, annot=True, fmt='.1f', cmap='YlGnBu', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Delay mean (ms) (seed-averaged)')\n",
    "\n",
    "    # Heatmap: Wake ratio mean\n",
    "    wake_pivot = comparison_df.pivot(index='middle_pattern', columns='outer_pattern', values='wake_ratio_mean')\n",
    "    sns.heatmap(wake_pivot, annot=True, fmt='.4f', cmap='YlGn', ax=axes[1, 0], vmin=0.005, vmax=0.01)\n",
    "    axes[1, 0].set_title('Wake ratio mean (seed-averaged)')\n",
    "\n",
    "    # Outer-pattern average performance summary\n",
    "    outer_summary = comparison_df.groupby('outer_pattern').agg({\n",
    "        'pdr_mean': 'mean',\n",
    "        'delay_mean': 'mean',\n",
    "        'wake_ratio_mean': 'mean'\n",
    "    })\n",
    "\n",
    "    outer_summary.plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Average performance by outer pattern (seed-averaged)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].legend(['PDR', 'Delay (ms)', 'Wake ratio'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    chart_path = \"~/workspace/analysis/triple_pattern_seed_averaged_comparison.png\"\n",
    "    plt.savefig(chart_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Outer-pattern impact for each middle pattern\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle('Outer-pattern impact by middle pattern (seed-averaged)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    middle_names = list(middle_scenario_configs.keys())\n",
    "    for i, middle_name in enumerate(middle_names):\n",
    "        if i >= 8:  # Up to 8 patterns\n",
    "            break\n",
    "\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "\n",
    "        middle_data = comparison_df[comparison_df['middle_pattern'] == middle_name]\n",
    "        if not middle_data.empty:\n",
    "            axes[row, col].bar(middle_data['outer_pattern'], middle_data['pdr_mean'])\n",
    "            axes[row, col].set_title(f'{middle_name}')\n",
    "            axes[row, col].set_ylabel('PDR (seed-averaged)')\n",
    "            axes[row, col].tick_params(axis='x', rotation=45)\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(len(middle_names), 8):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        axes[row, col].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    chart_path2 = \"~/workspace/analysis/middle_outer_effect_seed_averaged_analysis.png\"\n",
    "    plt.savefig(chart_path2, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSaved comparison plots:\")\n",
    "if num_outer == 1:\n",
    "    print(f\"  - Middle-pattern comparison: {chart_path}\")\n",
    "else:\n",
    "    print(f\"  - Three-layer comparison: {chart_path}\")\n",
    "    print(f\"  - Outer impact per middle pattern: {chart_path2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ns3-analysis",
   "language": "python",
   "name": "ns3-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
